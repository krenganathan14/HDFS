/*

AIM 	   :Hadoop common UseCase_Q&A
DATE	   :14-DEC-2017
PreRequest :hadoop-2.7.1/ RHEL Operation system.

*/

1.Create a new directory in linux namely ~/install/hdfsusecases and 
create a new file inside the above directory namely ~/install/hdfsusecases/NYSE_2020_06_20.txt 
copying the first 1000 lines from an existing file ~/pigdata/NYSE_daily

	$ cd ~/install/hdfsusecases
	$ head -1000 ~/pigdata/NYSE_daily > ~/install/hdfsusecases/NYSE_2020_06_20.txt
	

2. Create another new file inside the above directory namely ~/install/hdfsusecases/NYSE_2020_06_21.txt copying the line from 1001 to 2000 from an existing file ~/pigdata/NYSE_daily

	$ head -2000 ~/pigdata/NYSE_daily | tail -1000 > ~/install/hdfsusecases/NYSE_2020_06_21.txt
	
3.Create a directory in Hadoop namely /tmp/hdfsusecases
	$ hadoop fs -mkdir /tmp/hdfsusecases
	
4. Check whether the above directory is created in HDFS or not using the below command (Note: We use –test –d option to check whether the given path is a directory or not)
    
	$ hadoop fs -test -d /tmp/hdfsusecases

5.Check what is the status code of the above command using, if it shows 0 then directory is created, if shows non zero then the directory is not created then check the step 3 again.

    $ echo $?
	0
6.Copy file generated only in step 1 (~/install/hdfsusecases/NYSE_2020_06_20.txt) from linux to hdfs directory /tmp/hdfsusecases in the name of NYSE_2020_06.txt
    $ hadoop fs -put ~/install/hdfsusecases/NYSE_2020_06_20.txt /tmp/hdfsusecases/NYSE_2020_06.txt
	
7.Like step 4 and 5, check whether the above file (/tmp/hdfsusecases/NYSE_2020_06.txt) is created or not in HDFS, using f option and check for the 
  status code using $? and create a zero byte file in HDFS directory /tmp/hdfsusecases in the name of _SUCCESS
  
    $ hadoop fs -test -f /tmp/hdfsusecases/NYSE_2020_06.txt
    $ echo $?
    $ hadoop fs -touchz /tmp/hdfsusecases/_SUCCESS
	
8.Append the file generated in step 2 in linux (~/install/hdfsusecases/NYSE_2020_06_20.txt) with the file generated in step 6 
  in the hdfs directory /tmp/hdfsusecases/NYSE_2020_06.txt
  
	$ hadoop fs -appendToFile ~/install/hdfsusecases/NYSE_2020_06_21 /tmp/hdfsusecases/NYSE_2020_06.txt
	$ hadoop fs -appendToFile ~/install/hdfsusecases/NYSE_2020_06_21.txt /tmp/hdfsusecases/NYSE_2020_06.txt

9.Count the size of the file in HDFS /tmp/hdfsusecases/NYSE_2020_06.txt
	$ hadoop fs -du -h /tmp/hdfsusecases/NYSE_2020_06.txt
	
	
10.Count the number of rows are there in the /tmp/hdfsusecases/NYSE_2020_06.txt (Which should show the total count of the files created in step1 and 2)
	$ hadoop fs -cat /tmp/hdfsusecases/NYSE_2020_06.txt |wc -l

11.Display only line 11 to 20 from the file in HDFS /tmp/hdfsusecases/NYSE_2020_06.txt
	$ hadoop fs -cat /tmp/hdfsusecases/NYSE_2020_06.txt | head -20 | tail -10
	$ hadoop fs -cat /tmp/hdfsusecases/NYSE_2020_06.txt | sed -n '11,20p'  //it can avoid cat: Unable to write to output stream. warning message.


12.Store line 11 to 20 from the file in HDFS /tmp/hdfsusecases/NYSE_2020_06.txt into linux file namely ~/install/hdfsusecases/NYSE_sampledata1.txt
	hadoop fs -cat /tmp/hdfsusecases/NYSE_2020_06.txt | head -20 | tail -10 > ~/install/hdfsusecases/NYSE_sampledata1.txt  
	
13. Delete the line number 1 from the HDFS file /tmp/hdfsusecases/NYSE_2020_06.txt , for example if the above file contains 100 rows, after deletion it should have 
	only 99 rows in HDFS Note: we can’t do this directly because of the WORM property of HDFS data, think about the possible work around and try to achive the result
	
	$ hadoop fs -copyToLocal /tmp/hdfsusecases/NYSE_2020_06.txt  ~/install/hdfsusecases/NYSE_2020_06_bkp.txt
	$ sed '1d' ~/install/hdfsusecases/NYSE_2020_06_bkp.txt > ~/install/hdfsusecases/NYSE_2020_06_dfl.txt
	$ hadoop fs -copyFromLocal -f ~/install/hdfsusecases/NYSE_2020_06_dfl.txt  /tmp/hdfsusecases/NYSE_2020_06.txt
	$ hadoop fs -cat /tmp/hdfsusecases/NYSE_2020_06.txt |wc -l

14. Copy the above file /tmp/hdfsusecases/NYSE_2020_06.txt in the name of /tmp/hdfsusecases/NYSE_2020_06_bkp.txt
	$ hadoop fs -copyToLocal /tmp/hdfsusecases/NYSE_2020_06.txt  ~/install/hdfsusecases/NYSE_2020_06_bkp.txt
    
	$ hadoop fs -cp /tmp/hdfsusecases/NYSE_2020_06.txt /tmp/hdfsusecases/NYSE_2020_06_bkp.txt
	
15.Merge the files in HDFS /tmp/hdfsusecases/NYSE_2020_06.txt and /tmp/hdfsusecases/NYSE_2020_06_bkp.txt into Linux directory namely ~/install/hdfsusecases/NYSE_2020_06_merged.txt
   Note: We have to use the option called getmerge to achieve this as given below.
	
	$ hadoop fs getmerge /tmp/hdfsusecases/NYSE_2020_06_bkp.txt /tmp/hdfsusecases/NYSE_2020_06.txt ~/install/hdfsusecases/NYSE_2020_06_merged.txt
	
16.Set the blocksize 64MB while writing the file in HDFS, check in the UI how many blocks are generated
	
	//$ hadoop fs D dfs.block.size=67108864 put /home/hduser/install/hadoop-2.7.1.tar.gz /user/hduser/
	$ hadoop fs -D dfs.block.size=67108864 -put ~/install/hadoop-2.7.1.tar.gz /user/hduser/hadoop_64MbBLK-2.7.1.tar.gz
	
17. Set the blocksize 128MB (134217728) for the same file generated in step 16 and replace the existing file in HDFS.

	$ hadoop fs -D dfs.block.size=134217728 -put -f ~/install/hadoop-2.7.1.tar.gz /user/hduser/hadoop-2.7.1.tar.gz

18. Set the replication to 3 while writing the file in HDFS
	
	$ hadoop fs -D dfs.replication=3 -put -f /home/hduser/install/hadoop-2.7.1.tar.gz /user/hduser/

		
19. To check the block information (In which datanode block is present,no of blocks,size,replication, etc)

	$ hadoop fsck - -files -locations -blocks /user/hduser/hadoop-2.7.1.tar.gz
	
20. Important Command DistCp (distributed copy) is a tool used for copying data between one Hadoop cluster to another cluster or 
    with in the same cluster using mappers. (Interview Question how do you copy data from production Hadoop cluster to Dev Hadoop cluster)
	
	$ hadoop distcp hdfs://localhost:54310/user/hduser/hadoop-2.7.1.tar.gz hdfs://localhost:54310/user/hduser/hadoop/

	  //http://192.168.65.128:8088/ciuster/app / Cluster-RM
	  //http://192.168.65.128:50070/  //Cluster-DN&ND
	  
21. choose to overwrite the target files unconditionally even if it exists using upto 2 mappers depends

	$ hadoop distcp -overwrite hdfs://localhost:54310/user/hduser/hadoop-2.7.1.tar.gz hdfs://localhost:54310/user/hduser/hadoop/
	
22. To view the content of editlog file, need to convert into xml file using editlog viewer
	
	$ hdfs oev -i /usr/local/hadoop_store/hdfs/namenode/current/edits_inprogress_0000000000000000559 -o edittest.xml
